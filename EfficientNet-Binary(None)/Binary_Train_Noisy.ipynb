{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482698c5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68795670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "IDENTIFIER   = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "#-----------------------------------\n",
    "\n",
    "#numerical libs\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import PIL\n",
    "import cv2\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "# std libs\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import numbers\n",
    "import inspect\n",
    "import shutil\n",
    "from timeit import default_timer as timer\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "\n",
    "#from pprintpp import pprint, pformat\n",
    "import json\n",
    "import zipfile\n",
    "from shutil import copyfile\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "import sys\n",
    "from distutils.dir_util import copy_tree\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# constant #\n",
    "PI  = np.pi\n",
    "INF = np.inf\n",
    "EPS = 1e-12\n",
    "\n",
    "def seed_py(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8cfc5a",
   "metadata": {},
   "source": [
    "## Model and Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6cb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "mask_size = 64\n",
    "mask_size1 = 16\n",
    "data_dir = './'\n",
    "is_mixed_precision = True  #True #False\n",
    "output_dir = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc74732",
   "metadata": {},
   "source": [
    "## Defining Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c63f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://www.kaggle.com/lextoumbourou/radampytorch#radam.py\n",
    "#  https://forums.fast.ai/t/meet-ranger-radam-lookahead-optimizer/52886/21\n",
    "#  https://github.com/nachiket273/lookahead_pytorch\n",
    "#  https://github.com/mgrankin/over9000\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value = 1 - beta2)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group['lr'])\n",
    "                else:\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size * group['lr'])\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d282e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch libs\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "\n",
    "from torch.nn.utils.rnn import *\n",
    "\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad0a5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lib.include import *\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "import builtins\n",
    "import re\n",
    "\n",
    "class Struct(object):\n",
    "    def __init__(self, is_copy=False, **kwargs):\n",
    "        self.add(is_copy, **kwargs)\n",
    "\n",
    "    def add(self, is_copy=False, **kwargs):\n",
    "        #self.__dict__.update(kwargs)\n",
    "\n",
    "        if is_copy == False:\n",
    "            for key, value in kwargs.items():\n",
    "                setattr(self, key, value)\n",
    "        else:\n",
    "            for key, value in kwargs.items():\n",
    "                try:\n",
    "                    setattr(self, key, copy.deepcopy(value))\n",
    "                    #setattr(self, key, value.copy())\n",
    "                except Exception:\n",
    "                    setattr(self, key, value)\n",
    "\n",
    "    def drop(self,  missing=None, **kwargs):\n",
    "        drop_value = []\n",
    "        for key, value in kwargs.items():\n",
    "            try:\n",
    "                delattr(self, key)\n",
    "                drop_value.append(value)\n",
    "            except:\n",
    "                drop_value.append(missing)\n",
    "        return drop_value\n",
    "\n",
    "    def __str__(self):\n",
    "        text =''\n",
    "        for k,v in self.__dict__.items():\n",
    "            text += '\\t%s : %s\\n'%(k, str(v))\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "# log ------------------------------------\n",
    "def remove_comments(lines, token='#'):\n",
    "    \"\"\" Generator. Strips comments and whitespace from input lines.\n",
    "    \"\"\"\n",
    "\n",
    "    l = []\n",
    "    for line in lines:\n",
    "        s = line.split(token, 1)[0].strip()\n",
    "        if s != '':\n",
    "            l.append(s)\n",
    "    return l\n",
    "\n",
    "\n",
    "def open(file, mode=None, encoding=None):\n",
    "    if mode == None: mode = 'r'\n",
    "\n",
    "    if '/' in file:\n",
    "        if 'w' or 'a' in mode:\n",
    "            dir = os.path.dirname(file)\n",
    "            if not os.path.isdir(dir):  os.makedirs(dir)\n",
    "\n",
    "    f = builtins.open(file, mode=mode, encoding=encoding)\n",
    "    return f\n",
    "\n",
    "\n",
    "def remove(file):\n",
    "    if os.path.exists(file): os.remove(file)\n",
    "\n",
    "\n",
    "def empty(dir):\n",
    "    if os.path.isdir(dir):\n",
    "        shutil.rmtree(dir, ignore_errors=True)\n",
    "    else:\n",
    "        os.makedirs(dir)\n",
    "\n",
    "\n",
    "# http://stackoverflow.com/questions/34950201/pycharm-print-end-r-statement-not-working\n",
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        self.terminal = sys.stdout  #stdout\n",
    "        self.file = None\n",
    "\n",
    "    def open(self, file, mode=None):\n",
    "        if mode is None: mode ='w'\n",
    "        self.file = open(file, mode)\n",
    "\n",
    "    def write(self, message, is_terminal=1, is_file=1 ):\n",
    "        if '\\r' in message: is_file=0\n",
    "\n",
    "        if is_terminal == 1:\n",
    "            self.terminal.write(message)\n",
    "            self.terminal.flush()\n",
    "            #time.sleep(1)\n",
    "\n",
    "        if is_file == 1:\n",
    "            self.file.write(message)\n",
    "            self.file.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        # this flush method is needed for python 3 compatibility.\n",
    "        # this handles the flush command by doing nothing.\n",
    "        # you might want to specify some extra behavior here.\n",
    "        pass\n",
    "\n",
    "# io ------------------------------------\n",
    "def write_list_to_file(list_file, strings):\n",
    "    with open(list_file, 'w') as f:\n",
    "        for s in strings:\n",
    "            f.write('%s\\n'%str(s))\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_list_from_file(list_file, comment='#'):\n",
    "    with open(list_file) as f:\n",
    "        lines  = f.readlines()\n",
    "    strings=[]\n",
    "    for line in lines:\n",
    "        if comment is not None:\n",
    "            s = line.split(comment, 1)[0].strip()\n",
    "        else:\n",
    "            s = line.strip()\n",
    "        if s != '':\n",
    "            strings.append(s)\n",
    "    return strings\n",
    "\n",
    "\n",
    "\n",
    "def read_pickle_from_file(pickle_file):\n",
    "    with open(pickle_file,'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    return x\n",
    "\n",
    "def write_pickle_to_file(pickle_file, x):\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(x, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "# backup ------------------------------------\n",
    "\n",
    "#https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory\n",
    "def backup_project_as_zip(project_dir, zip_file):\n",
    "    assert(os.path.isdir(project_dir))\n",
    "    assert(os.path.isdir(os.path.dirname(zip_file)))\n",
    "    shutil.make_archive(zip_file.replace('.zip',''), 'zip', project_dir)\n",
    "    pass\n",
    "\n",
    "\n",
    "# etc ------------------------------------\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def np_float32_to_uint8(x, scale=255):\n",
    "    return (x*scale).astype(np.uint8)\n",
    "\n",
    "def np_uint8_to_float32(x, scale=255):\n",
    "    return (x/scale).astype(np.float32)\n",
    "\n",
    "\n",
    "def int_tuple(x):\n",
    "    return tuple( [int(round(xx)) for xx in x] )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def df_loc_by_list(df, key, values):\n",
    "    df = df.loc[df[key].isin(values)]\n",
    "    df = df.assign(sort = pd.Categorical(df[key], categories=values, ordered=True))\n",
    "    df = df.sort_values('sort')\n",
    "    #df = df.reset_index()\n",
    "    df = df.drop('sort', axis=1)\n",
    "    return  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05485200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "def np_loss_binary_cross_entropy(probability, truth):\n",
    "    batch_size = len(probability)\n",
    "    probability = probability.reshape(-1)\n",
    "    truth = truth.reshape(-1)\n",
    "\n",
    "    log_p_pos = -np.log(np.clip(probability,1e-5,1))\n",
    "    log_p_neg = -np.log(np.clip(1-probability,1e-5,1))\n",
    "\n",
    "    loss = log_p_pos[truth==1].sum() + log_p_neg[truth==0].sum()\n",
    "    loss = loss/len(truth)\n",
    "    return loss\n",
    "\n",
    "def np_metric_map_curve_by_class(truth,probability):\n",
    "    num_sample, num_label = probability.shape\n",
    "    score = []\n",
    "    for i in range(num_label):\n",
    "        s = average_precision_score(truth[:,i], probability[:,i])\n",
    "        score.append(s)\n",
    "    score = np.array(score)\n",
    "    return score\n",
    "\n",
    "def np_metric_roc_auc(probability, truth):\n",
    "    truth = truth.reshape(-1)\n",
    "    probability = probability.reshape(-1)\n",
    "    score = roc_auc_score(truth, probability)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4535cd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpytorch\n",
      "\t\tseed = 1629378109\n",
      "\t\ttorch.__version__              = 1.8.1+cu102\n",
      "\t\ttorch.version.cuda             = 10.2\n",
      "\t\ttorch.backends.cudnn.version() = 7605\n",
      "\t\tos['CUDA_VISIBLE_DEVICES']     = None\n",
      "\t\ttorch.cuda.device_count()      = 1\n",
      "\t\ttorch.cuda.get_device_properties() = (name='Tesla V100-SXM2-32GB', major=7, minor=0, total_memory=32510MB, multi_processor_count=80)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "COMMON_STRING = ''\n",
    "if 1:\n",
    "    seed = int(time.time())\n",
    "    seed_py(seed)\n",
    "    seed_torch(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark     = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled       = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    COMMON_STRING += '\\tpytorch\\n'\n",
    "    COMMON_STRING += '\\t\\tseed = %d\\n'%seed\n",
    "    COMMON_STRING += '\\t\\ttorch.__version__              = %s\\n'%torch.__version__\n",
    "    COMMON_STRING += '\\t\\ttorch.version.cuda             = %s\\n'%torch.version.cuda\n",
    "    COMMON_STRING += '\\t\\ttorch.backends.cudnn.version() = %s\\n'%torch.backends.cudnn.version()\n",
    "    try:\n",
    "        COMMON_STRING += '\\t\\tos[\\'CUDA_VISIBLE_DEVICES\\']     = %s\\n'%os.environ['CUDA_VISIBLE_DEVICES']\n",
    "        NUM_CUDA_DEVICES = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))\n",
    "    except Exception:\n",
    "        COMMON_STRING += '\\t\\tos[\\'CUDA_VISIBLE_DEVICES\\']     = None\\n'\n",
    "        NUM_CUDA_DEVICES = 1\n",
    "\n",
    "    COMMON_STRING += '\\t\\ttorch.cuda.device_count()      = %d\\n'%torch.cuda.device_count()\n",
    "    COMMON_STRING += '\\t\\ttorch.cuda.get_device_properties() = %s\\n' % str(torch.cuda.get_device_properties(0))[21:]\n",
    "\n",
    "COMMON_STRING += '\\n'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print (COMMON_STRING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9a64f",
   "metadata": {},
   "source": [
    "## Defining Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb6fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--- flip ---\n",
    "def do_random_hflip(image, mask):\n",
    "    if np.random.rand()>0.5:\n",
    "        image = cv2.flip(image,1)\n",
    "        mask = cv2.flip(mask,1)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "# #--- geometric ---\n",
    "def do_random_rotate(image, mask, mag=15 ):\n",
    "    angle = np.random.uniform(-1, 1)*mag\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    cx, cy = width // 2, height // 2\n",
    "\n",
    "    transform = cv2.getRotationMatrix2D((cx, cy), -angle, 1.0)\n",
    "    image = cv2.warpAffine(image, transform, (width, height), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    mask = cv2.warpAffine(mask, transform, (width, height), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def do_random_scale( image, mask, mag=0.1 ):\n",
    "    s = 1 + np.random.uniform(-1, 1)*mag\n",
    "    height, width = image.shape[:2]\n",
    "    w,h = int(s*width), int(s*height)\n",
    "    if (h,w)==image.shape[:2]:\n",
    "        return image, mask\n",
    "\n",
    "    dst = np.array([\n",
    "        [0,0],[width,height], [width,0], #[0,height],\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "    if s>1:\n",
    "        dx = np.random.choice(w-width)\n",
    "        dy = np.random.choice(h-height)\n",
    "        src = np.array([\n",
    "            [-dx,-dy],[-dx+w,-dy+h], [-dx+w,-dy],#[-dx,-dy+h],#\n",
    "        ]).astype(np.float32)\n",
    "    if s<1:\n",
    "        dx = np.random.choice(width-w)\n",
    "        dy = np.random.choice(height-h)\n",
    "        src = np.array([\n",
    "            [dx,dy], [dx+w,dy+h], [dx+w,dy],#\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "    transform = cv2.getAffineTransform(src, dst)\n",
    "    image = cv2.warpAffine( image, transform, (width, height), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    mask = cv2.warpAffine( mask, transform, (width, height), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def do_random_stretch_y( image, mask, mag=0.25 ):\n",
    "    s = 1 + np.random.uniform(-1, 1)*mag\n",
    "    height, width = image.shape[:2]\n",
    "    h = int(s*height)\n",
    "    w = width\n",
    "    if h==height:\n",
    "        return image, mask\n",
    "\n",
    "    dst = np.array([\n",
    "        [0,0],[width,height], [width,0], #[0,height],\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "\n",
    "    if s>1:\n",
    "        dx = 0#np.random.choice(w-width)\n",
    "        dy = np.random.choice(h-height)\n",
    "        src = np.array([\n",
    "            [-dx,-dy],[-dx+w,-dy+h], [-dx+w,-dy],#[-dx,-dy+h],#\n",
    "        ]).astype(np.float32)\n",
    "    if s<1:\n",
    "        dx = 0#np.random.choice(width-w)\n",
    "        dy = np.random.choice(height-h)\n",
    "        src = np.array([\n",
    "            [dx,dy], [dx+w,dy+h], [dx+w,dy],#\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "    transform = cv2.getAffineTransform(src, dst)\n",
    "    image = cv2.warpAffine( image, transform, (width, height), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    mask = cv2.warpAffine( mask, transform, (width, height), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "\n",
    "def do_random_stretch_x( image, mask, mag=0.25 ):\n",
    "    s = 1 + np.random.uniform(-1, 1)*mag\n",
    "    height, width = image.shape[:2]\n",
    "    h = height\n",
    "    w = int(s*width)\n",
    "    if w==width:\n",
    "        return image, mask\n",
    "\n",
    "    dst = np.array([\n",
    "        [0,0],[width,height], [width,0], #[0,height],\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "    if s>1:\n",
    "        dx = np.random.choice(w-width)\n",
    "        dy = 0#np.random.choice(h-height)\n",
    "        src = np.array([\n",
    "            [-dx,-dy],[-dx+w,-dy+h], [-dx+w,-dy],#[-dx,-dy+h],#\n",
    "        ]).astype(np.float32)\n",
    "    if s<1:\n",
    "        dx = np.random.choice(width-w)\n",
    "        dy = 0#np.random.choice(height-h)\n",
    "        src = np.array([\n",
    "            [dx,dy], [dx+w,dy+h], [dx+w,dy],#\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "    transform = cv2.getAffineTransform(src, dst)\n",
    "    image = cv2.warpAffine( image, transform, (width, height), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    mask = cv2.warpAffine( mask, transform, (width, height), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def do_random_shift( image, mask, mag=32 ):\n",
    "    b = mag\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    image = cv2.copyMakeBorder(image, b,b,b,b, borderType=cv2.BORDER_CONSTANT, value=0)\n",
    "    mask  = cv2.copyMakeBorder(mask, b,b,b,b, borderType=cv2.BORDER_CONSTANT, value=0)\n",
    "    x = np.random.randint(0,2*b)\n",
    "    y = np.random.randint(0,2*b)\n",
    "    image = image[y:y+height,x:x+width]\n",
    "    mask = mask[y:y+height,x:x+width]\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "###########################################################################################3\n",
    "\n",
    "\n",
    "\n",
    "# #--- noise ---\n",
    "def do_random_blurout(image, size=0.20, num_cut=3):\n",
    "    height, width = image.shape[:2]\n",
    "    size = int(size*(height+width)/2)\n",
    "    for t in range(num_cut):\n",
    "        x = np.random.randint(0,width- size)\n",
    "        y = np.random.randint(0,height-size)\n",
    "        x0 = x\n",
    "        x1 = x+size\n",
    "        y0 = y\n",
    "        y1 = y+size\n",
    "        image[y0:y1,x0:x1]=image[y0:y1,x0:x1].mean()\n",
    "\n",
    "    return image\n",
    "\n",
    "def do_random_guassian_blur(image, mag=[0.1, 2.0]):\n",
    "    sigma = np.random.uniform(mag[0],mag[1])\n",
    "    image = cv2.GaussianBlur(image, (23, 23), sigma)\n",
    "    return image\n",
    "\n",
    "def do_random_noise(image, mag=0.08):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    image = image.astype(np.float32)/255\n",
    "    noise = np.random.uniform(-1,1,size=(height,width))*mag\n",
    "    image = image+noise\n",
    "\n",
    "    image = np.clip(image,0,1)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "# # --- intensity ---\n",
    "def do_random_intensity_shift_contast(image, mag=[0.3,0.2]):\n",
    "    image = (image).astype(np.float32)/255\n",
    "    alpha0 = 1 + random.uniform(-1,1)*mag[0]\n",
    "    alpha1 = random.uniform(-1,1)*mag[1]\n",
    "    image = (image+alpha1)\n",
    "    image = np.clip(image,0,1)\n",
    "    image = image**alpha0\n",
    "    image = np.clip(image,0,1)\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "#https://answers.opencv.org/question/12024/use-of-clahe/)\n",
    "def do_random_clahe(image, mag=[[2,4],[6,12]]):\n",
    "    l = np.random.uniform(*mag[0])\n",
    "    g = np.random.randint(*mag[1])\n",
    "    clahe = cv2.createCLAHE(clipLimit=l, tileGridSize=(g, g))\n",
    "\n",
    "    image = clahe.apply(image)\n",
    "    return image\n",
    "\n",
    "# https://github.com/facebookresearch/CovidPrognosis/blob/master/covidprognosis/data/transforms.py\n",
    "def do_histogram_norm(image, mag=[[2,4],[6,12]]):\n",
    "    num_bin = 255\n",
    "\n",
    "    histogram, bin = np.histogram( image.flatten(), num_bin, density=True)\n",
    "    cdf = histogram.cumsum()  # cumulative distribution function\n",
    "    cdf = 255 * cdf / cdf[-1]  # normalize\n",
    "\n",
    "    # use linear interpolation of cdf to find new pixel values\n",
    "    equalized = np.interp(image.flatten(), bin[:-1], cdf)\n",
    "    image = equalized.reshape(image.shape)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45320bd",
   "metadata": {},
   "source": [
    "## Dataloader Class and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f1252ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_fold(mode='train-1'):\n",
    "    if 'train' in mode:\n",
    "        df_study = pd.read_csv(data_dir+'/df_study_split_binary_negative_eb5ns_eb6eb6_ns_4024.csv')\n",
    "        df_study['set'] = \"train\"\n",
    "        df = df_study.copy()\n",
    "\n",
    "        fold = int(mode[-1])\n",
    "        df_train = df[df.fold != fold].reset_index(drop=True)\n",
    "        df_valid = df[df.fold == fold].reset_index(drop=True)\n",
    "        return df_train, df_valid\n",
    "\n",
    "    if 'test' in mode:\n",
    "        print(\"Please use Inference Pipeline\")\n",
    "\n",
    "def null_augment(r):\n",
    "    image = r['image']\n",
    "    return r\n",
    "\n",
    "\n",
    "class SiimDataset(Dataset):\n",
    "    def __init__(self, df, augment=null_augment):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "        self.length = len(df)\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen = %d\\n'%len(self)\n",
    "        string += '\\tdf  = %s\\n'%str(self.df.shape)\n",
    "\n",
    "        string += '\\tlabel distribution\\n'\n",
    "        for i in range(2):\n",
    "            if i == 0 :\n",
    "                n = self.df['none'].sum()\n",
    "                n = len(self.df) - n\n",
    "            if i == 1:\n",
    "                n = self.df['none'].sum()\n",
    "            string += '\\t\\t %d %26s: %5d (%0.4f)\\n'%(i, 'none', n, n/len(self.df) )\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.df.iloc[index]\n",
    "        image_file = data_dir+'/%s/%s.png' % (d.set, d.image)\n",
    "        image = cv2.resize(cv2.imread(image_file,cv2.IMREAD_GRAYSCALE), (image_size, image_size))\n",
    "        onehot = d[['none']].values\n",
    "        onehotns = d[['negative']].values\n",
    "\n",
    "        if d.set == 'train':\n",
    "            mask_file = data_dir+'/%s_mask/%s.png' % (d.set, d.image)\n",
    "            mask = cv2.imread(mask_file,cv2.IMREAD_GRAYSCALE)\n",
    "            try:\n",
    "                mask = cv2.resize(mask, (image_size,image_size))\n",
    "            except:\n",
    "                mask = np.zeros_like(image)\n",
    "        else:\n",
    "            mask = np.zeros_like(image)\n",
    "\n",
    "        r = {\n",
    "            'index' : index,\n",
    "            'd' : d,\n",
    "            'image' : image,\n",
    "            'mask' : mask,\n",
    "            'onehot' : onehot,\n",
    "            'onehotns' : onehotns,\n",
    "        }\n",
    "        if self.augment is not None: r = self.augment(r)\n",
    "        return r\n",
    "\n",
    "\n",
    "def null_collate(batch):\n",
    "    collate = defaultdict(list)\n",
    "\n",
    "    for r in batch:\n",
    "        for k, v in r.items():\n",
    "            collate[k].append(v)\n",
    "\n",
    "    # ---\n",
    "    batch_size = len(batch)\n",
    "    onehot = np.ascontiguousarray(np.stack(collate['onehot'])).astype(np.float32)\n",
    "    collate['onehot'] = torch.from_numpy(onehot)\n",
    "    onehotns = np.ascontiguousarray(np.stack(collate['onehotns'])).astype(np.float32)\n",
    "    collate['onehotns'] = torch.from_numpy(onehotns)\n",
    "\n",
    "    image = np.stack(collate['image'])\n",
    "    image = image.reshape(batch_size, 1, image_size,image_size).repeat(3,1)\n",
    "    image = np.ascontiguousarray(image)\n",
    "    image = image.astype(np.float32) / 255\n",
    "    collate['image'] = torch.from_numpy(image)\n",
    "\n",
    "\n",
    "    mask = np.stack(collate['mask'])\n",
    "    mask = mask.reshape(batch_size, 1, image_size,image_size)\n",
    "    mask = np.ascontiguousarray(mask)\n",
    "    mask = mask.astype(np.float32) / 255\n",
    "    collate['mask'] = torch.from_numpy(mask)\n",
    "\n",
    "    return collate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699e579",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c63b5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.efficientnet import *\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        e = tf_efficientnet_b6_ns(pretrained=True, drop_rate=0.5, drop_path_rate=0.3)\n",
    "        self.b0 = nn.Sequential(\n",
    "            e.conv_stem,\n",
    "            e.bn1,\n",
    "            e.act1,\n",
    "        )\n",
    "        self.b1 = e.blocks[0]\n",
    "        self.b2 = e.blocks[1]\n",
    "        self.b3 = e.blocks[2]\n",
    "        self.b4 = e.blocks[3]\n",
    "        self.b5 = e.blocks[4]\n",
    "        self.b6 = e.blocks[5]\n",
    "        self.b7 = e.blocks[6]\n",
    "        self.b8 = nn.Sequential(\n",
    "            e.conv_head, #for eb3 384, 1536 #for eb7 640,2560\n",
    "            e.bn2,\n",
    "            e.act2,\n",
    "        )\n",
    "\n",
    "        self.logit = nn.Linear(2304,1)\n",
    "        self.mask = nn.Sequential(\n",
    "            nn.Conv2d(72, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 1, kernel_size=1, padding=0),\n",
    "        )\n",
    "        \n",
    "        self.mask1 = nn.Sequential(\n",
    "            nn.Conv2d(344, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 1, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "\n",
    "    # @torch.cuda.amp.autocast()\n",
    "    def forward(self, image):\n",
    "        batch_size = len(image)\n",
    "        x = 2*image-1     # ; print('input ',   x.shape)\n",
    "\n",
    "        x = self.b0(x) #; print (x.shape)  # torch.Size([2, 40, 256, 256])\n",
    "        x = self.b1(x) #; print (x.shape)  # torch.Size([2, 24, 256, 256])\n",
    "        x = self.b2(x) #; print (x.shape)  # torch.Size([2, 32, 128, 128])\n",
    "        x = self.b3(x) #; print (x.shape)  # torch.Size([2, 48, 64, 64])\n",
    "        mask = self.mask(x)\n",
    "        x = self.b4(x) #; print (x.shape)  # torch.Size([2, 96, 32, 32])\n",
    "        x = self.b5(x) #; print (x.shape)  # torch.Size([2, 136, 32, 32])\n",
    "        #------------\n",
    "        #-------------\n",
    "        x = self.b6(x) #; print (x.shape)  # torch.Size([2, 232, 16, 16])\n",
    "        mask1 = self.mask1(x)\n",
    "        x = self.b7(x) #; print (x.shape)  # torch.Size([2, 384, 16, 16])\n",
    "        x = self.b8(x) #; print (x.shape)  # torch.Size([2, 1536, 16, 16])\n",
    "        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n",
    "        logit = self.logit(x)\n",
    "        return logit, mask, mask1\n",
    "\n",
    "# check #################################################################\n",
    "\n",
    "def run_check_net():\n",
    "    batch_size = 2\n",
    "    C, H, W = 3, 512, 512\n",
    "    image = torch.randn(batch_size, C, H, W).cuda()\n",
    "    mask  = torch.randn(batch_size, 1, H, W).cuda()\n",
    "\n",
    "    net = Net().cuda()\n",
    "    logit, mask, mask1 = net(image)\n",
    "\n",
    "    print(image.shape)\n",
    "    print(logit.shape)\n",
    "    print(mask.shape)\n",
    "    print(mask1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3433fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Loss function Libarary\n",
    "from pytorch_toolbelt.losses import (\n",
    "    BinaryLovaszLoss,\n",
    "    DiceLoss,\n",
    "    BinaryFocalLoss,\n",
    ")\n",
    "from pytorch_toolbelt.losses import BinaryFocalLoss, JointLoss\n",
    "#----------------\n",
    "import os\n",
    "import torch.cuda.amp as amp\n",
    "from madgrad import MADGRAD\n",
    "import torch.optim as optim\n",
    "\n",
    "class AmpNet(Net):\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def forward(self,*args):\n",
    "        return super(AmpNet, self).forward(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4307ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_augment(r):\n",
    "    image = r['image']\n",
    "    mask = r['mask']\n",
    "\n",
    "    if 1:\n",
    "        for fn in np.random.choice([\n",
    "            lambda image, mask : do_random_scale(image, mask, mag=0.30),\n",
    "            lambda image, mask : do_random_stretch_y(image, mask, mag=0.30),\n",
    "            lambda image, mask : do_random_stretch_x(image, mask, mag=0.30),\n",
    "            lambda image, mask : do_random_shift(image, mask, mag=int(0.30*image_size)),\n",
    "            lambda image, mask : (image, mask)\n",
    "        ],4):\n",
    "            image, mask = fn(image, mask)\n",
    "\n",
    "        for fn in np.random.choice([\n",
    "            lambda image, mask : do_random_rotate(image, mask, mag=20),\n",
    "            lambda image, mask : do_random_hflip(image, mask),\n",
    "            lambda image, mask : (image, mask)\n",
    "        ],3):\n",
    "            image, mask = fn(image, mask)\n",
    "\n",
    "        # ------------------------\n",
    "        for fn in np.random.choice([\n",
    "            lambda image : do_random_intensity_shift_contast(image, mag=[0.5,0.5]),\n",
    "            lambda image : do_random_noise(image, mag=0.30),\n",
    "            lambda image : do_random_guassian_blur(image),\n",
    "            lambda image : do_random_blurout(image, size=0.30, num_cut=2),\n",
    "            lambda image : do_histogram_norm(image),\n",
    "            lambda image : image,\n",
    "        ],4):\n",
    "            image = fn(image)\n",
    "\n",
    "    r['image'] = image\n",
    "    r['mask'] = mask\n",
    "    return r\n",
    "\n",
    "\n",
    "\n",
    "def do_valid(net, valid_loader, criterion):\n",
    "\n",
    "    valid_probability = []\n",
    "    valid_truth = []\n",
    "    valid_num = 0\n",
    "\n",
    "    net.eval()\n",
    "    start_timer = timer()\n",
    "    for t, batch in enumerate(valid_loader):\n",
    "        batch_size = len(batch['index'])\n",
    "        image = batch['image'].cuda()\n",
    "        label = batch['onehot']\n",
    "\n",
    "        with torch.no_grad():\n",
    "                logit, mask, mask1 = net(image)\n",
    "                probability = logit.sigmoid()\n",
    "\n",
    "        valid_num += batch_size\n",
    "        valid_probability.append(probability.data.cpu().numpy())\n",
    "        valid_truth.append(label.data.cpu().numpy())\n",
    "        print('\\r %8d / %d  %s'%(valid_num, len(valid_loader.dataset),time_to_str(timer() - start_timer,'sec')),end='',flush=True)\n",
    "\n",
    "    assert(valid_num == len(valid_loader.dataset))\n",
    "\n",
    "    truth = np.concatenate(valid_truth)\n",
    "    probability = np.concatenate(valid_probability)\n",
    "    predict = probability.argsort(-1)[::-1]\n",
    "\n",
    "    loss = np_loss_binary_cross_entropy(probability,truth)\n",
    "\n",
    "    mapp  = np_metric_map_curve_by_class(truth, probability)*(1/6)\n",
    "    auc = np_metric_roc_auc(probability, truth)\n",
    "\n",
    "    return [loss, auc, mapp]\n",
    "\n",
    "\n",
    "\n",
    "# start here ! ###################################################################################\n",
    "\n",
    "\n",
    "def run_train():\n",
    "\n",
    "    for fold in [0,1,2,3,4]:\n",
    "        out_dir = output_dir+'result/eb6ns_512_binary_noisy/fold%d'%fold\n",
    "        #initial_checkpoint = output_dir+'result/eb6ns_512_binary_noisy/fold%d/checkpoint/best_model_map.pth'%fold\n",
    "        initial_checkpoint = None\n",
    "        \n",
    "        start_iteration = 0\n",
    "        \n",
    "        if initial_checkpoint is not None:\n",
    "            f = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n",
    "            start_iteration = f['iteration']\n",
    "\n",
    "        start_lr   = 0.0001\n",
    "        batch_size = 16\n",
    "\n",
    "        num_iteration = start_iteration + 50\n",
    "        iter_log    = 50\n",
    "        iter_valid  = 50\n",
    "        iter_save   = list(range(0, num_iteration+1, 50))\n",
    "\n",
    "        ## setup  ----------------------------------------\n",
    "        for f in ['checkpoint', 'train', 'valid', 'backup']: os.makedirs(out_dir + '/' + f, exist_ok=True)\n",
    "\n",
    "        log = Logger()\n",
    "        log.open(out_dir + '/log.train.txt', mode='a')\n",
    "        log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n",
    "        log.write('\\t%s\\n' % COMMON_STRING)\n",
    "        log.write('\\tout_dir  = %s\\n' % out_dir)\n",
    "        log.write('\\n')\n",
    "\n",
    "        ## dataset ------------------------------------\n",
    "        df_train, df_valid = make_fold('train-%d'%fold)\n",
    "        train_dataset = SiimDataset(df_train, train_augment)\n",
    "        valid_dataset = SiimDataset(df_valid, )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size,\n",
    "            drop_last   = True,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n",
    "            collate_fn  = null_collate,\n",
    "        )\n",
    "        valid_loader  = DataLoader(\n",
    "            valid_dataset,\n",
    "            sampler = SequentialSampler(valid_dataset),\n",
    "            batch_size  = batch_size,\n",
    "            drop_last   = False,\n",
    "            num_workers = 4,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate,\n",
    "        )\n",
    "\n",
    "        log.write('train_dataset : \\n%s\\n'%(train_dataset))\n",
    "        log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n",
    "        log.write('\\n')\n",
    "\n",
    "\n",
    "        ## net ----------------------------------------\n",
    "        log.write('** net setting **\\n')\n",
    "        if is_mixed_precision:\n",
    "            scaler = amp.GradScaler()\n",
    "            net = AmpNet().cuda()\n",
    "        else:\n",
    "            net = Net().cuda()\n",
    "\n",
    "\n",
    "        if initial_checkpoint is not None:\n",
    "            f = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n",
    "            start_iteration = f['iteration']\n",
    "            start_epoch = f['epoch']\n",
    "            state_dict  = f['state_dict']\n",
    "            net.load_state_dict(state_dict,strict=True)  #True\n",
    "        else:\n",
    "            start_iteration = 0\n",
    "            start_epoch = 0\n",
    "        \n",
    "        \n",
    "\n",
    "        log.write('net=%s\\n'%(type(net)))\n",
    "        log.write('\\tinitial_checkpoint = %s\\n' % initial_checkpoint)\n",
    "        log.write('\\n')\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        lovasz_loss=BinaryLovaszLoss()\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            log.write(\"Let's use %d GPUs! \\n\" % (torch.cuda.device_count()))\n",
    "            net = nn.DataParallel(net)\n",
    "\n",
    "\n",
    "        optimizer = RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=start_lr)\n",
    "        #optimizer = MADGRAD( filter(lambda p: p.requires_grad, net.parameters()), lr=start_lr, momentum= 0.9, weight_decay= 1e-06, eps= 1e-06)\n",
    "        \n",
    "        log.write('optimizer\\n  %s\\n'%(optimizer))\n",
    "        log.write('\\n')\n",
    "\n",
    "\n",
    "        ## start training here! ##############################################\n",
    "        log.write('** start training here! **\\n')\n",
    "        log.write('   fold = %d\\n'%(fold))\n",
    "        log.write('   is_mixed_precision = %s \\n'%str(is_mixed_precision))\n",
    "        log.write('   batch_size = %d\\n'%(batch_size))\n",
    "        log.write('                             |----- VALID --------|---- TRAIN/BATCH --------------\\n')\n",
    "        log.write('rate        iter    epoch    | loss    AUC    MAP | loss0  loss1  loss2 | time          \\n')\n",
    "        log.write('----------------------------------------------------------------------\\n')\n",
    "                  #0.00000   0.00* 0.00  | 0.000  0.000  | 0.000  0.000  |  0 hr 00 min\n",
    "\n",
    "        def message(mode='print'):\n",
    "            if mode==('print'):\n",
    "                asterisk = ' '\n",
    "                loss = batch_loss\n",
    "            if mode==('log'):\n",
    "                asterisk = '*' if iteration in iter_save else ' '\n",
    "                loss = train_loss\n",
    "\n",
    "            text = \\\n",
    "                '%0.7f  %5.4f%s %4.2f  | '%(rate, iteration/10000, asterisk, epoch,) +\\\n",
    "                '%4.4f  %4.5f  %4.5f  | '%(*valid_loss,) +\\\n",
    "                '%4.3f  %4.3f  %4.3f  | '%(*loss,) +\\\n",
    "                '%s' % (time_to_str(timer() - start_timer,'min'))\n",
    "\n",
    "            return text\n",
    "\n",
    "        #----\n",
    "        valid_loss = np.zeros(3,np.float32)\n",
    "        train_loss = np.zeros(3,np.float32)\n",
    "        batch_loss = np.zeros_like(train_loss)\n",
    "        sum_train_loss = np.zeros_like(train_loss)\n",
    "        sum_train = 0\n",
    "        loss0 = torch.FloatTensor([0]).cuda().sum()\n",
    "        loss1 = torch.FloatTensor([0]).cuda().sum()\n",
    "        loss2 = torch.FloatTensor([0]).cuda().sum()\n",
    "\n",
    "\n",
    "        start_timer = timer()\n",
    "        iteration = start_iteration\n",
    "        epoch = start_epoch\n",
    "        rate = 0\n",
    "        auc_metric = 0\n",
    "        map_metric = 0\n",
    "        while  iteration < num_iteration:\n",
    "\n",
    "            for t, batch in enumerate(train_loader):\n",
    "\n",
    "                if (iteration % iter_valid == 0):\n",
    "                        valid_loss = do_valid(net, valid_loader, criterion)  #\n",
    "                        auc_val = valid_loss[1]\n",
    "                        map_val = valid_loss[2]\n",
    "                        if auc_val >  auc_metric:\n",
    "                            auc_metric = auc_val\n",
    "                            if iteration in iter_save:\n",
    "                                if iteration != start_iteration:\n",
    "                                    torch.save({\n",
    "                                        'state_dict': net.state_dict(),\n",
    "                                        'iteration': iteration,\n",
    "                                        'epoch': epoch,\n",
    "                                    }, out_dir + '/checkpoint/best_model_auc.pth')\n",
    "                        if map_val >  map_metric:\n",
    "                            map_metric = map_val\n",
    "                            if iteration in iter_save:\n",
    "                                if iteration != start_iteration:\n",
    "                                    torch.save({\n",
    "                                        'state_dict': net.state_dict(),\n",
    "                                        'iteration': iteration,\n",
    "                                        'epoch': epoch,\n",
    "                                    }, out_dir + '/checkpoint/best_model_map.pth')\n",
    "                                    pass\n",
    "                        pass\n",
    "\n",
    "                if (iteration % iter_log == 0):\n",
    "                    print('\\r', end='', flush=True)\n",
    "                    log.write(message(mode='log') + '\\n')\n",
    "\n",
    "                rate = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "                # one iteration update  -------------\n",
    "                batch_size = len(batch['index'])\n",
    "                image = batch['image'].cuda()\n",
    "                truthmask = batch['mask'].cuda()\n",
    "                truth_mask = F.interpolate(truthmask, size=(mask_size,mask_size), mode='bilinear', align_corners=False)\n",
    "                truth_mask1 = F.interpolate(truthmask, size=(mask_size1,mask_size1), mode='bilinear', align_corners=False)\n",
    "                label = batch['onehot'].cuda()\n",
    "                labelns = batch['onehotns'].cuda()\n",
    "\n",
    "                #----\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if is_mixed_precision:\n",
    "                    with amp.autocast():\n",
    "                        logit, mask, mask1 = net(image)\n",
    "                        loss0 = criterion(logit, label)\n",
    "                        loss1 = lovasz_loss(mask, truth_mask)\n",
    "                        loss2 = lovasz_loss(mask1, truth_mask1)\n",
    "                        distill_loss = F.binary_cross_entropy_with_logits(logit, labelns)\n",
    "                        cls_loss = loss0 * (1 - 0.5) + distill_loss * 0.5\n",
    "\n",
    "                    scaler.scale(cls_loss+loss1+loss2).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                # print statistics  --------\n",
    "                epoch += 1 / len(train_loader)\n",
    "                iteration += 1\n",
    "\n",
    "                batch_loss = np.array([loss0.item(), loss1.item(), loss2.item()])\n",
    "                sum_train_loss += batch_loss\n",
    "                sum_train += 1\n",
    "                if iteration % 100 == 0:\n",
    "                    train_loss = sum_train_loss / (sum_train + 1e-12)\n",
    "                    sum_train_loss[...] = 0\n",
    "                    sum_train = 0\n",
    "\n",
    "                print('\\r', end='', flush=True)\n",
    "                print(message(mode='print'), end='', flush=True)\n",
    "        log.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a548f824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
